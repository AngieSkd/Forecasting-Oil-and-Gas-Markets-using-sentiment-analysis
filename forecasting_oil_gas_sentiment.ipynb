{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce389a3",
   "metadata": {},
   "source": [
    "\n",
    "# Dissertation Example (Cleaned Version)\n",
    "\n",
    "This notebook demonstrates a workflow for **data management, compilation, and forecasting**.  \n",
    "Note: All data shown here are "synthetic" and included only for demonstration.  \n",
    "Original licensed data have been removed and replaced with generated examples.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2537555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "#Basic libraries\n",
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "import seaborn as sns                            # plots\n",
    "import matplotlib.dates as mdates                # for dates\n",
    "from dateutil.relativedelta import relativedelta # working with dates with style\n",
    "\n",
    "#Statistical tools\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import pacf,acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from itertools import product\n",
    "from sklearn.model_selection import TimeSeriesSplit                  \n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#Models\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from statsmodels.tsa.forecasting.theta import ThetaModel\n",
    "\n",
    "# Important libraries for text and sentiment analysis\n",
    "import re\n",
    "import unicodedata\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "from bs4 import BeautifulSoup \n",
    "import html\n",
    "\n",
    "# Forecasting metrics\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e26003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Excel:\n",
    "stoxx = pd.read_excel(\"synthetic_prices.csv.xlsx\", parse_dates=[\"DATE\"])\n",
    "stoxx['DATE'] = pd.to_datetime(stoxx['DATE'])\n",
    "stoxx.set_index(\"DATE\", inplace=True)\n",
    "\n",
    "# Resample to weekly data, calculating average weekly price\n",
    "# 'W-FRI' means week ending on Friday\n",
    "weekly_avg = stoxx['PRICE'].resample('W-FRI').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f4b6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_avg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b5a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the time series so that weekly observations are labeled with the Monday date corresponding to each week\n",
    "weekly_avg.index = weekly_avg.index - pd.Timedelta(days=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65564a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd5ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_avg.info()\n",
    "weekly_avg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7249c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"The N/A values equal: \", weekly_avg.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88cfb1",
   "metadata": {},
   "source": [
    "# Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fad1a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=weekly_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf6efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoxx.plot(y='PRICE',figsize=(10,5))\n",
    "plt.title('STOXX 600 Europe Oil & Gas Index (daily data)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "859fc7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.plot(y='PRICE',figsize=(10,5))\n",
    "plt.title('STOXX 600 Europe Oil & Gas Index (average weekly prices)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b110f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_month=stoxx.resample(\"M\").mean()\n",
    "ts_month.plot(figsize=(10,5))\n",
    "plt.title('STOXX 600 Europe Oil & Gas Index (monthly resampling)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b92ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resample daily data to yearly (using mean price per year)\n",
    "yearly_df = weekly_avg.resample('Y').mean()  # Alternatives: .last(), .max(), etc.\n",
    "yearly_df.index = yearly_df.index.year\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(yearly_df.index, yearly_df.values, marker='o', linestyle='-', color='steelblue', linewidth=2, markersize=6)\n",
    "\n",
    "plt.title('Yearly Average Price of STOXX 600 Oil & Gas Index', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Price', fontsize=14)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# show every 3rd year on x-axis\n",
    "years_to_show = yearly_df.index[::3]\n",
    "plt.xticks(years_to_show, years_to_show.astype(int), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c006be",
   "metadata": {},
   "source": [
    "# Time series decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034716b",
   "metadata": {},
   "source": [
    "Assuming 52 weeks during a year\n",
    "\n",
    "Add the magnitude of each component. Also create a plot that shows the seasonality within a year/every month It is expected that during winter time prices increase and during summer drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4156b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(ts, model='multiplicative',period=52)\n",
    "# Plot the decomposed components\n",
    "plt.figure(figsize=(13, 9))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(ts, label='Original Data')\n",
    "plt.title('Original Time Series')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(decomposition.trend, label='Trend Component')\n",
    "plt.title('Trend Component')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(decomposition.seasonal, label='Seasonal Component')\n",
    "plt.title('Seasonal Component')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(decomposition.resid, label='Residual Component')\n",
    "plt.title('Residual Component')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db892f0",
   "metadata": {},
   "source": [
    "# ADF (Augmented Dickeyâ€“Fuller) test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e09685",
   "metadata": {},
   "source": [
    "Rolling window:\n",
    "\n",
    "1) Statistical interpetation: Pandas dataframe.rolling() function provides the feature of rolling window calculations. In very simple words we take a window size of k at a time and perform some desired mathematical operation on it. A window of size k means k consecutive values at a time. In a very simple case, all the 'k' values are equally weighted. ~ In case of rolling with window e.g. 3, the average of the previous 2 and the current price is calculated. This average represents the new average price.\n",
    "\n",
    "2) Financial interpretation: A moving average (MA) is a stock indicator commonly used in technical analysis, used to help smooth out price data by creating a constantly updated average price. A rising moving average indicates that the security is in an uptrend, while a declining moving average indicates a downtrend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb931468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(ts):\n",
    "    rolling_mean=ts.rolling(window=52).mean()\n",
    "    rolling_var=ts.rolling(window=52).std()\n",
    "    actual_ts=plt.plot(ts,color=\"black\",label=\"Actual\")\n",
    "    mean=plt.plot(rolling_mean,color=\"red\",label=\"Rolling mean\")\n",
    "    stddev=plt.plot(rolling_var,color=\"blue\",label=\"Rolling variance\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Rolling mean & standard deviation\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Show ADF test output\n",
    "    print(\"Showing ADF test output\")\n",
    "    adf_test_result=adfuller(ts,autolag=\"AIC\")\n",
    "    adf_output=pd.Series(adf_test_result[0:4],index=[\"Test statistic\", \"p-value\",\"#Lags used\",\"Number of observations used\"])\n",
    "    print(\"p-value: \", round(adf_test_result[1],4))\n",
    "    for key,value in adf_test_result[4].items():\n",
    "        adf_output[\"Critical value (%s)\" %key]=value\n",
    "    print(adf_output)\n",
    "    \n",
    "check_stationarity(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48fc474",
   "metadata": {},
   "source": [
    "Given that p=1.5% < 5% , the time series is stationary at 5% level of significance and non-stationary at 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d85217",
   "metadata": {},
   "source": [
    "# KPSS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5deee1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPSS test for level stationarity\n",
    "kpss_stat, p_value, lags, crit_vals = kpss(ts, regression='c')  # 'c' for constant\n",
    "print('KPSS Statistic:', kpss_stat)\n",
    "print('p-value:', p_value)\n",
    "print('Critical Values:', crit_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71622d",
   "metadata": {},
   "source": [
    "Given that p=2.25% < 5% , the time series is non-stationary at 5% level of significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36eaf9f",
   "metadata": {},
   "source": [
    "# PACF & ACF for raw prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24b3c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(2,1,1)\n",
    "plot_acf(ts,lags=20, ax=plt.gca())\n",
    "plt.title(\"ACF for prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a4a1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(2,1,1)\n",
    "plot_pacf(ts,lags=20, ax=plt.gca())\n",
    "plt.title(\"PACF for prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e298ff88",
   "metadata": {},
   "source": [
    "# Differentiate (remove trends) and log transformation (stabilize variance) to make time series stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08113056",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_log=np.log(weekly_avg)\n",
    "ts_log_dif=ts_log-ts_log.shift()\n",
    "ts_log_dif.plot(x='DATE', y='log_returns', figsize=(10,5))\n",
    "ts_log_dif.dropna(inplace=True)\n",
    "plt.title('STOXX 600 Europe Oil & Gas Index Log Returns')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36bfca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the time series to log returns \n",
    "ts_log_dif = ts_log_dif.rename('LOG_RETURN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d4268ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(ts_log_dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2192b1",
   "metadata": {},
   "source": [
    "Given that p=0% < 5% , the time series is stationary.\n",
    "\n",
    "Below, we check time series decomposition again on log transformed prices.\n",
    "\n",
    "Multiplicative decompositions are common with economic time series. When a log transformation has been used, this is equivalent to using a multiplicative decomposition on the original data because yt=St TtRt is equivalent to logyt = logSt + logTt +logRt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "238ef4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPSS test for level stationarity\n",
    "kpss_stat, p_value, lags, crit_vals = kpss(ts_log_dif, regression='c')  # 'c' for constant, 'ct' for trend\n",
    "\n",
    "print('KPSS Statistic:', kpss_stat)\n",
    "print('p-value:', p_value)\n",
    "print('Critical Values:', crit_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8893c1b6",
   "metadata": {},
   "source": [
    "Given that p-value=10%>5%, it can be concluded that log returns time series is stationary.\n",
    "\n",
    "KPSS and ADF tests agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1c01f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(ts_log_dif, model='additive',period=52)\n",
    "\n",
    "plt.rcParams.update({'axes.titlesize': 23,'axes.labelsize': 23,'xtick.labelsize': 18,'ytick.labelsize': 18,'legend.fontsize': 18})\n",
    "\n",
    "# Plot the decomposed components\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(ts, label='Original Data')\n",
    "plt.title('Original Time Series')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(decomposition.trend, label='Trend Component')\n",
    "plt.title('Trend Component')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(decomposition.seasonal, label='Seasonal Component')\n",
    "plt.title('Seasonal Component')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(decomposition.resid, label='Residual Component')\n",
    "plt.title('Residual Component')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868acec",
   "metadata": {},
   "source": [
    "# PACF & ACF plots for log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f8a4ebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'axes.titlesize': 15,'axes.labelsize': 15,'xtick.labelsize': 15,'ytick.labelsize': 15,'legend.fontsize': 15})\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(2,1,1)\n",
    "plot_acf(ts_log_dif,lags=10, ax=plt.gca())\n",
    "plt.title(\"ACF for log returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1751e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(2,1,1)\n",
    "plot_pacf(ts_log_dif,lags=10, ax=plt.gca())\n",
    "plt.title(\"PACF for log returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb30971",
   "metadata": {},
   "source": [
    "# Extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ab9fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = ts_log_dif.to_frame().copy()\n",
    "ts_df.columns = ['LOG_RETURN'] \n",
    "ts_df['year'] = ts_df.index.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc32a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c0f2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = ts_df['LOG_RETURN'].quantile(0.25)\n",
    "Q3 = ts_df['LOG_RETURN'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "ts_df['outlier_iqr'] = (ts_df['LOG_RETURN'] < lower) | (ts_df['LOG_RETURN'] > upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f62ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IQR\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(ts_df.index, ts_df['LOG_RETURN'], label='Weekly Log Returns', alpha=0.7)\n",
    "plt.scatter(ts_df.index[ts_df['outlier_iqr']], \n",
    "            ts_df.loc[ts_df['outlier_iqr'], 'LOG_RETURN'], \n",
    "            color='red', label='Outliers')\n",
    "plt.title(\"Outliers in Weekly Log Returns (IQR technique)\")\n",
    "plt.ylabel(\"Log Return\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "048b8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IQR technique\n",
    "# Count outliers per year\n",
    "yearly_outlier_counts = ts_df.groupby('year')['outlier_iqr'].sum()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "yearly_outlier_counts.plot(kind='bar', color='crimson')\n",
    "plt.title(\"Number of Outliers in Weekly Returns per Year\")\n",
    "plt.ylabel(\"Outlier Count\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "981643e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of outliers\n",
    "yearly_outlier_counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc04b9dd",
   "metadata": {},
   "source": [
    "# Create a new csv file with log transformed and differentiated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23adbecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = ts_log_dif.copy()\n",
    "\n",
    "# Reset index and format date\n",
    "df_to_save = df_to_save.reset_index()\n",
    "df_to_save[\"DATE\"] = df_to_save[\"DATE\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Save\n",
    "df_to_save.to_excel(\"synthetic_weekly_returns.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19639b05",
   "metadata": {},
   "source": [
    "# Read the headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac0dcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel\n",
    "headlines=pd.read_excel('synthetic_headlines.csv')\n",
    "\n",
    "# Load and convert\n",
    "headlines['DATE'] = pd.to_datetime(headlines['DATE'])\n",
    "\n",
    "# Compute weekday: Monday=0, Sunday=6\n",
    "headlines['weekday'] = headlines['DATE'].dt.weekday\n",
    "\n",
    "# For Mon-Fri (0â€“4), subtract weekday to get Monday\n",
    "# For Sat/Sun (5â€“6), subtract weekday, then add 7 days to get next Monday\n",
    "headlines['WEEK_STARTING'] = headlines['DATE'] - pd.to_timedelta(headlines['weekday'], unit='d')\n",
    "headlines.loc[headlines['weekday'] >= 5, 'WEEK_STARTING'] += pd.Timedelta(days=7)\n",
    "\n",
    "# Drop helper column\n",
    "headlines = headlines.drop(columns=['weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d732407",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c1ae615",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['WEEK_STARTING'] = pd.to_datetime(headlines['WEEK_STARTING'])\n",
    "\n",
    "headlines = headlines.set_index('WEEK_STARTING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78ff1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only relevant articles\n",
    "KEYWORDS = ['crude oil', 'natural gas', 'opec', 'oil prices', 'gas prices', 'oil', 'gas', 'OPEC']\n",
    "\n",
    "# Create a regex pattern\n",
    "pattern = '|'.join(KEYWORDS)\n",
    "\n",
    "# Keep only rows where the headline contains at least one keyword (case-insensitive)\n",
    "headlines_filtered = headlines[headlines['headline'].str.contains(pattern, case=False, na=False)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cf83052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Unescape HTML entities (e.g., &amp; â†’ &)\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    \n",
    "    # Remove non-printable/control characters\n",
    "    text = ''.join(c for c in text if c.isprintable())\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2dea3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered[\"headline\"] = headlines_filtered[\"headline\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7cbd76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\",local_files_only=False)\n",
    "model = BertForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\",local_files_only=False)\n",
    "\n",
    "# Function to compute sentiment score\n",
    "def finbert_sentiment(text):\n",
    "    # Truncate long headlines to avoid exceeding token limits\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    probs = softmax(outputs.logits, dim=1)\n",
    "    # probs[0]: tensor with 3 probabilities: negative, neutral, positive\n",
    "    # We create a sentiment score: positive - negative\n",
    "    sentiment_score = (probs[0][2] - probs[0][0]).item()\n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77045ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8862df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered=headlines_filtered.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91ff3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7b16096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename index\n",
    "headlines_filtered = headlines_filtered.rename_axis(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ac9bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9738b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentiment\n",
    "# Convert Series to DataFrame\n",
    "headlines_filtered = headlines_filtered.to_frame(name=\"headline\")\n",
    "\n",
    "headlines_filtered[\"sentiment_score\"] = headlines_filtered[\"headline\"].apply(finbert_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fe7ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate weekly sentiment\n",
    "weekly_sentiment = (headlines_filtered.groupby('DATE').agg({'sentiment_score': 'mean'}).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9dcefb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check headlines_filtered\n",
    "weekly_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7620d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sentiment.rename(columns={\"WEEK_STARTING\": \"DATE\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b5d52",
   "metadata": {},
   "source": [
    "# Descriptive statistics for sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96e1e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of sentiment score\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(weekly_sentiment, kde=True, color='blue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentiment Score')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff2020",
   "metadata": {},
   "source": [
    "# Create a new csv file with weekly sentiment and headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a60de799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save weekly sentiment\n",
    "df_to_save1 = weekly_sentiment.copy()\n",
    "\n",
    "# Reset index and format date\n",
    "df_to_save1 = df_to_save1.reset_index()\n",
    "df_to_save1[\"DATE\"] = df_to_save1[\"DATE\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Save \n",
    "df_to_save1.to_excel(\"synthetic_weekly_sentiment.csv\", index=False)\n",
    "\n",
    "\n",
    "#Save headlines\n",
    "df_to_save2 = headlines_filtered.copy()\n",
    "\n",
    "# Reset index and format date\n",
    "df_to_save2 = df_to_save2.reset_index()\n",
    "df_to_save2[\"DATE\"] = df_to_save2[\"DATE\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Save\n",
    "df_to_save2.to_excel(\"synthetic_headlines.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0b927",
   "metadata": {},
   "source": [
    "# Combine sentiment with STOXX returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86bd711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOXX=pd.read_excel('synthetic_weekly_returns.csv')\n",
    "WEEKLY_SENTIMENT=pd.read_excel('synthetic_weekly_sentiment.csv')\n",
    "HEADLINES=pd.read_excel('synthetic_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ca6e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOXX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "710165d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEKLY_SENTIMENT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "70a8513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADLINES.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5a34729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DATE column to datetime\n",
    "STOXX['DATE'] = pd.to_datetime(STOXX['DATE'])\n",
    "#Set \"WEEK_STARTING\" as the index\n",
    "STOXX = STOXX.set_index('DATE')\n",
    "\n",
    "# Convert DATE column to datetime\n",
    "WEEKLY_SENTIMENT['DATE'] = pd.to_datetime(WEEKLY_SENTIMENT['DATE'])\n",
    "#Set \"WEEK_STARTING\" as the index\n",
    "WEEKLY_SENTIMENT = WEEKLY_SENTIMENT.set_index('DATE')\n",
    "\n",
    "# Convert DATE column to datetime\n",
    "HEADLINES['DATE'] = pd.to_datetime(HEADLINES['DATE'])\n",
    "#Set \"WEEK_STARTING\" as the index\n",
    "HEADLINES = HEADLINES.set_index('DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6116914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEKLY_SENTIMENT=WEEKLY_SENTIMENT.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d6caf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEEKLY_SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00bac253",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_returns = pd.merge(STOXX, WEEKLY_SENTIMENT, on='DATE', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "920570e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2fda65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_returns.isnull().sum()\n",
    "\n",
    "#Plot missing values \n",
    "df = pd.DataFrame(merged_returns)\n",
    "# Filter only the 'sentiment_score' column\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df[['sentiment_score']].isna(),  cbar=False, cmap='viridis',  yticklabels=False, \n",
    "            xticklabels=['sentiment_score'])\n",
    "plt.title('Missing Values in Sentiment Score')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Date Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2263c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_returns.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bd38b",
   "metadata": {},
   "source": [
    "We can see consecutive weeks (possibly more than 2-3 weeks in a row) with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cbba7d",
   "metadata": {},
   "source": [
    "# Fill na values with mean of last weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2908dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rolling=merged_returns.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66dfa9",
   "metadata": {},
   "source": [
    "In case of consecutive weeks with no headlines where the mean would equal N/A, the N/A value is filled with previous last observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f368158",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rolling[\"sentiment_score\"] = merged_rolling[\"sentiment_score\"].fillna(\n",
    "    merged_rolling[\"sentiment_score\"].rolling(window=2, min_periods=1).mean()).fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "738bf49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rolling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f381aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first row with NaN\n",
    "merged_rolling=merged_rolling.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05f64cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check n/a values\n",
    "merged_rolling.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e358df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rolling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b714e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rolling.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6587469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram with light blue bars and KDE line\n",
    "light_blue = sns.color_palette()[0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(\n",
    "    merged_rolling[\"sentiment_score\"],\n",
    "    kde=True,\n",
    "    color=light_blue,\n",
    "    alpha=0.5,\n",
    "    edgecolor='black',\n",
    "    label='sentiment_score')\n",
    "\n",
    "# Add labels, title, legend\n",
    "plt.xlabel(\"Sentiment Score (filled)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Sentiment Score (4-week Rolling Mean Imputation)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4805abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count of articles per week\n",
    "weekly_article_count = HEADLINES.groupby('DATE').size().rename(\"article_count\")\n",
    "\n",
    "# 2. Merge article count into merged_rolling\n",
    "merged_rolling = merged_rolling.merge(weekly_article_count, how='left', left_index=True, right_index=True)\n",
    "merged_rolling[\"article_count\"] = merged_rolling[\"article_count\"].fillna(0)\n",
    "\n",
    "# 3. Compute signal = sentiment_score * article_count\n",
    "merged_rolling[\"signal\"] = merged_rolling[\"sentiment_score\"] * merged_rolling[\"article_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d0d86c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rolling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "edc66483",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(merged_rolling[['sentiment_score','signal','LOG_RETURN']].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation between Sentiment Scores and Market Direction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008070d",
   "metadata": {},
   "source": [
    "Signal has the highest correlation with log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c99d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variables' statistics\n",
    "merged_rolling.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd992e9",
   "metadata": {},
   "source": [
    "Check correlation with signal and its lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6c440fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rolling[\"signal_lag1\"] = merged_rolling[\"signal\"].shift(1)\n",
    "merged_rolling[\"signal_lag2\"] = merged_rolling[\"signal\"].shift(2)\n",
    "merged_rolling[\"signal_lag3\"] = merged_rolling[\"signal\"].shift(3)\n",
    "merged_rolling[\"signal_lag4\"] = merged_rolling[\"signal\"].shift(4)\n",
    "merged_rolling[\"LOG_RETURN_lag1\"] = merged_rolling[\"LOG_RETURN\"].shift(1)\n",
    "merged_rolling[\"LOG_RETURN_lag2\"] = merged_rolling[\"LOG_RETURN\"].shift(2)\n",
    "merged_rolling[\"LOG_RETURN_lag3\"] = merged_rolling[\"LOG_RETURN\"].shift(3)\n",
    "merged_rolling[\"LOG_RETURN_lag4\"] = merged_rolling[\"LOG_RETURN\"].shift(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3a794d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))  # figure size\n",
    "sns.heatmap(\n",
    "    merged_rolling[[\"signal\", \"signal_lag1\", \"signal_lag2\", \"signal_lag3\", \"signal_lag3\", \n",
    "                    \"LOG_RETURN_lag1\", \"LOG_RETURN_lag2\", \"LOG_RETURN_lag3\", \"LOG_RETURN_lag4\", \"LOG_RETURN\"]].corr(),\n",
    "    annot=True,                # Show numbers\n",
    "    fmt=\".2f\",                 # Format to 2 decimal places\n",
    "    annot_kws={\"size\": 10},    # Larger font size\n",
    "    cmap=\"coolwarm\",           # Better color contrast\n",
    "    center=0,                  # Center colors at 0\n",
    "    linewidths=0.5,            # Add borders to cells\n",
    "    vmin=-1, vmax=1)            # Ensure full color range\n",
    "\n",
    "plt.title('Correlation between Signal Scores and Log Returns', pad=20)  # Add padding to title\n",
    "plt.xticks(rotation=45)        # Rotate x-axis labels for readability\n",
    "plt.yticks(rotation=0)         # Keep y-axis labels horizontal\n",
    "plt.tight_layout()             # Adjust layout to prevent clipping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1afc55c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "# You may need to adjust lags, up to 5 weeks for example\n",
    "max_lag = 5\n",
    "grangercausalitytests(merged_rolling[['signal','LOG_RETURN' ]], maxlag=max_lag, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f06e3c9",
   "metadata": {},
   "source": [
    "We can conclude that signal Granger cause log return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89410733",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rolling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3dcc44b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unwanted columns\n",
    "merged_rolling=merged_rolling.iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c37e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rolling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aaf00005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize both series to z-scores\n",
    "normalized = merged_rolling[['LOG_RETURN', 'signal']].copy()\n",
    "normalized['LOG_RETURN_z'] = (normalized['LOG_RETURN'] - normalized['LOG_RETURN'].mean()) / normalized['LOG_RETURN'].std()\n",
    "normalized['signal_z'] = (normalized['signal'] - normalized['signal'].mean()) / normalized['signal'].std()\n",
    "\n",
    "# Plot normalized series\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(normalized.index, normalized['LOG_RETURN_z'], label='LOG_RETURN (z-score)',\n",
    "         color='blue', linewidth=1.5, alpha=0.8)\n",
    "plt.plot(normalized.index, normalized['signal_z'], label='Signal (Sentiment Ã— Count, z-score)',\n",
    "         color='orange', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "plt.title(\"Normalized Signal vs LOG_RETURN\", fontsize=16)\n",
    "plt.xlabel(\"Date\", fontsize=13)\n",
    "plt.ylabel(\"Z-score\", fontsize=13)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "\n",
    "plt.ylim(-6, 6)\n",
    "\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(2))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9d50da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 1327\n",
    "n_splits = 5\n",
    "fold_size = n_obs // (n_splits + 1)\n",
    "window_size = 120\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 3))\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    test_start = fold_size * (fold + 1)\n",
    "    test_end = test_start + fold_size\n",
    "\n",
    "    test_range = np.arange(test_start, test_end)\n",
    "    train_range = np.arange(test_start - window_size, test_start)\n",
    "\n",
    "    ax.plot(train_range, [fold + 1]*len(train_range), color=\"blue\", label=\"Train\" if fold == 0 else \"\")\n",
    "    ax.plot(test_range, [fold + 1]*len(test_range), color=\"orange\", label=\"Test\" if fold == 0 else \"\")\n",
    "\n",
    "ax.set_yticks(range(1, n_splits+1))\n",
    "ax.set_yticklabels([f\"Fold {i+1}\" for i in range(n_splits)])\n",
    "ax.set_xlabel(\"Observation Index\")\n",
    "ax.set_title(\"TimeSeriesSplit with Rolling Window (120)\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3dee7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_rolling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a516f",
   "metadata": {},
   "source": [
    "Save final time series table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb0d22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save weekly sentiment\n",
    "df_to_save3 = merged_rolling.copy()\n",
    "\n",
    "# Reset index and format date\n",
    "df_to_save3 = df_to_save3.reset_index()\n",
    "df_to_save3[\"DATE\"] = df_to_save3[\"DATE\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Save \n",
    "df_to_save3.to_excel(\"final_timeseries.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1337ca4",
   "metadata": {},
   "source": [
    "# Random Walk as benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2aa211a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and clean data\n",
    "ts_df = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\"])\n",
    "\n",
    "# Storage\n",
    "all_preds_rw = []\n",
    "all_true = []\n",
    "\n",
    "# TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(ts_df)):\n",
    "    train = ts_df.iloc[train_idx].copy()\n",
    "    test = ts_df.iloc[test_idx].copy()\n",
    "    \n",
    "    # Predict next return as last observed return\n",
    "    full = pd.concat([train, test])\n",
    "    full[\"RW_PRED\"] = full[\"LOG_RETURN\"].shift(1)\n",
    "\n",
    "    preds = full.loc[test.index, \"RW_PRED\"]\n",
    "    trues = full.loc[test.index, \"LOG_RETURN\"]\n",
    "\n",
    "    # Drop NaNs\n",
    "    valid_idx = ~preds.isna()\n",
    "    preds = preds[valid_idx]\n",
    "    trues = trues[valid_idx]\n",
    "\n",
    "    all_preds_rw.extend(preds)\n",
    "    all_true.extend(trues)\n",
    "\n",
    "    if fold == tscv.n_splits - 1:\n",
    "        last_fold_preds = preds\n",
    "        last_fold_true = trues\n",
    "        last_fold_index = test.index[valid_idx]\n",
    "\n",
    "# Evaluation\n",
    "rmse_rw = np.sqrt(mean_squared_error(all_true, all_preds_rw))\n",
    "mse_rw = mean_squared_error(all_true, all_preds_rw)\n",
    "mae_rw = mean_absolute_error(all_true, all_preds_rw)\n",
    "\n",
    "print(f\"Random Walk (1-week ahead): RMSE = {rmse_rw:.5f}, MSE = {mse_rw:.5f}, MAE = {mae_rw:.5f}\")\n",
    "\n",
    "# Plot Last Fold\n",
    "full_rw_preds = np.full(len(ts_df), np.nan)\n",
    "pred_positions = ts_df.index.get_indexer(last_fold_index)\n",
    "full_rw_preds[pred_positions] = last_fold_preds.values\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(ts_df.index, ts_df[\"LOG_RETURN\"], label=\"True Returns\", color=\"blue\")\n",
    "plt.plot(ts_df.index, full_rw_preds, label=\"Random Walk Forecast (last fold)\", color=\"orange\")\n",
    "plt.axvline(ts_df.index[pred_positions[0]], color=\"black\", linestyle=\"--\", label=\"Last Fold Start\")\n",
    "plt.title(\"Random Walk Forecast (1-week ahead, Last Fold)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Log Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe04c4a",
   "metadata": {},
   "source": [
    "# Random Walk for multiple horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9b8096d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define forecasting horizons\n",
    "forecast_horizons = [5, 22, 65]\n",
    "\n",
    "# Copy and clean data\n",
    "ts_df = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\"])\n",
    "\n",
    "# Loop through each forecasting horizon\n",
    "for h in forecast_horizons:\n",
    "    all_preds_rw = []\n",
    "    all_true = []\n",
    "\n",
    "    # TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(ts_df)):\n",
    "        train = ts_df.iloc[train_idx].copy()\n",
    "        test = ts_df.iloc[test_idx].copy()\n",
    "        \n",
    "        # Combine and shift LOG_RETURN by h steps to simulate RW forecast\n",
    "        full = pd.concat([train, test])\n",
    "        full[f\"RW_PRED_h{h}\"] = full[\"LOG_RETURN\"].shift(h)\n",
    "\n",
    "        # Get predictions and actuals for horizon h\n",
    "        preds = full.loc[test.index, f\"RW_PRED_h{h}\"]\n",
    "        trues = full.loc[test.index, \"LOG_RETURN\"]\n",
    "\n",
    "        # Drop NaNs (due to shifting)\n",
    "        valid_idx = ~preds.isna()\n",
    "        preds = preds[valid_idx]\n",
    "        trues = trues[valid_idx]\n",
    "\n",
    "        all_preds_rw.extend(preds)\n",
    "        all_true.extend(trues)\n",
    "\n",
    "        if fold == tscv.n_splits - 1:\n",
    "            last_fold_preds = preds\n",
    "            last_fold_true = trues\n",
    "            last_fold_index = test.index[valid_idx]\n",
    "\n",
    "    # Evaluation\n",
    "    rmse_rw = np.sqrt(mean_squared_error(all_true, all_preds_rw))\n",
    "    mse_rw = mean_squared_error(all_true, all_preds_rw)\n",
    "    mae_rw = mean_absolute_error(all_true, all_preds_rw)\n",
    "\n",
    "    print(f\"Random Walk ({h}-week ahead): RMSE = {rmse_rw:.5f}, MSE = {mse_rw:.5f}, MAE = {mae_rw:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80e6ae",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4d918d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Parameters\n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "h = 1  # one-step-ahead forecast\n",
    "\n",
    "# Prepare data\n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\"])\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Storage\n",
    "all_preds_arima_h1 = []\n",
    "all_true_arima_h1 = []\n",
    "last_fold_preds = []\n",
    "last_fold_true = []\n",
    "last_fold_index = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(data)):\n",
    "    full_df = data.iloc[np.concatenate([train_idx, test_idx])].copy()\n",
    "\n",
    "    for i in test_idx:\n",
    "        start_idx = i - window_size\n",
    "        forecast_idx = i + h - 1\n",
    "\n",
    "        if start_idx < 0 or forecast_idx >= len(full_df):\n",
    "            continue\n",
    "\n",
    "        train_series = full_df.iloc[start_idx:i][\"LOG_RETURN\"]\n",
    "\n",
    "        try:\n",
    "            model = ARIMA(train_series, order=(1, 0, 0))\n",
    "            fitted = model.fit()\n",
    "            forecast = fitted.forecast(steps=h)[-1]\n",
    "        except:\n",
    "            forecast = np.nan\n",
    "\n",
    "        true_value = full_df.iloc[forecast_idx][\"LOG_RETURN\"]\n",
    "\n",
    "        if not np.isnan(forecast):\n",
    "            all_preds_arima_h1.append(forecast)\n",
    "            all_true_arima_h1.append(true_value)\n",
    "\n",
    "            if fold == n_splits - 1:\n",
    "                last_fold_preds.append(forecast)\n",
    "                last_fold_true.append(true_value)\n",
    "                last_fold_index.append(full_df.index[forecast_idx])\n",
    "\n",
    "# Metrics\n",
    "rmse_arima_h1 = np.sqrt(mean_squared_error(all_true_arima_h1, all_preds_arima_h1))\n",
    "mse_arima_h1 = mean_squared_error(all_true_arima_h1, all_preds_arima_h1)\n",
    "mae_arima_h1 = mean_absolute_error(all_true_arima_h1, all_preds_arima_h1)\n",
    "\n",
    "print(f\"ARIMA(1,0,0) Forecast (h=1):\")\n",
    "print(f\"RMSE: {rmse_arima_h1:.5f}, MSE: {mse_arima_h1:.5f}, MAE: {mae_arima_h1:.5f}\")\n",
    "\n",
    "# Plot Last Fold Forecast\n",
    "full_preds = np.full(len(data), np.nan)\n",
    "idx_positions = data.index.get_indexer(last_fold_index)\n",
    "full_preds[idx_positions] = last_fold_preds\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(data.index, data[\"LOG_RETURN\"].values, label=\"True Returns\", color=\"blue\")\n",
    "plt.plot(data.index, full_preds, label=\"ARIMA (h=1) Forecast\", color=\"orange\")\n",
    "plt.axvline(data.index[idx_positions[0]], color=\"black\", linestyle=\"--\", label=\"Last Fold Start\")\n",
    "plt.title(\"ARIMA(1,0,0) One-Step Ahead Forecast (Last Fold)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Log Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea0042",
   "metadata": {},
   "source": [
    "ARIMA for multiple horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d26bbf59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Parameters\n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "horizons = [1, 5, 22, 65]  # forecasting steps ahead\n",
    "\n",
    "# Prepare data\n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\", \"signal\"])\n",
    "\n",
    "# Storage\n",
    "results_arima = {}\n",
    "\n",
    "for h in horizons:\n",
    "    all_preds, all_true = [], []\n",
    "    last_fold_preds, last_fold_true, last_fold_index = [], [], []\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    splits = list(tscv.split(data))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in tqdm(enumerate(splits), total=n_splits, desc=f\"ARIMA h={h}\"):\n",
    "        full_df = data.iloc[np.concatenate([train_idx, test_idx])].copy()\n",
    "\n",
    "        for i in test_idx:\n",
    "            start_idx = i - window_size\n",
    "            forecast_idx = i + h - 1  # target index is t+h\n",
    "\n",
    "            if start_idx < 0 or forecast_idx >= len(full_df):\n",
    "                continue\n",
    "\n",
    "            train_series = full_df.iloc[start_idx:i][\"LOG_RETURN\"]\n",
    "\n",
    "            try:\n",
    "                model = ARIMA(train_series, order=(1, 0, 0))\n",
    "                fitted = model.fit()\n",
    "                forecast = fitted.forecast(steps=h)[-1]\n",
    "            except:\n",
    "                forecast = np.nan\n",
    "\n",
    "            true_value = full_df.iloc[forecast_idx][\"LOG_RETURN\"]\n",
    "\n",
    "            if not np.isnan(forecast):\n",
    "                all_preds.append(forecast)\n",
    "                all_true.append(true_value)\n",
    "\n",
    "                if fold == n_splits - 1:\n",
    "                    last_fold_preds.append(forecast)\n",
    "                    last_fold_true.append(true_value)\n",
    "                    last_fold_index.append(full_df.index[forecast_idx])\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(all_true, all_preds))\n",
    "    mse = mean_squared_error(all_true, all_preds)\n",
    "    mae = mean_absolute_error(all_true, all_preds)\n",
    "\n",
    "    results_arima[h] = {\"RMSE\": rmse,\"MSE\": mse,\"MAE\": mae,\"last_fold_preds\": last_fold_preds,\n",
    "        \"last_fold_true\": last_fold_true,\"last_fold_index\": last_fold_index}\n",
    "\n",
    "    print(f\"\\nARIMA Forecast - Horizon {h} steps:\")\n",
    "    print(f\"RMSE: {rmse:.5f}, MSE: {mse:.5f}, MAE: {mae:.5f}\")\n",
    "\n",
    "    # Plot Last Fold\n",
    "    full_preds = np.full(len(data), np.nan)\n",
    "    idx_positions = data.index.get_indexer(last_fold_index)\n",
    "    full_preds[idx_positions] = last_fold_preds\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(data.index, data[\"LOG_RETURN\"].values, label=\"True Returns\", color=\"blue\")\n",
    "    plt.plot(data.index, full_preds, label=f\"ARIMA Forecast (h={h}, last fold)\", color=\"orange\")\n",
    "    plt.axvline(data.index[idx_positions[0]], color=\"black\", linestyle=\"--\", label=\"Last Fold Start\")\n",
    "    plt.title(f\"ARIMA(1,0,0) Forecast - Horizon {h}-step ahead (Last Fold)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Log Return\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44bf93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f08f6357",
   "metadata": {},
   "source": [
    "# Theta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "75ef0d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Suppress statsmodels frequency warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Parameters\n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "horizons = [1, 5, 22, 65]\n",
    "\n",
    "# Prepare data \n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\", \"signal\"])  \n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "splits = list(tscv.split(data))\n",
    "\n",
    "# Storage\n",
    "results = []\n",
    "\n",
    "# Forecast\n",
    "for h in horizons:\n",
    "    print(f\"\\n Theta Forecast: {h}-Week Ahead \")\n",
    "\n",
    "    all_preds, all_trues = [], []\n",
    "    last_fold_preds = []\n",
    "    last_fold_index = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "        concat_idx = np.concatenate([train_idx, test_idx])\n",
    "        full_df = data.iloc[concat_idx].copy()\n",
    "\n",
    "        # positions inside full_df\n",
    "        test_start_pos = len(train_idx)\n",
    "\n",
    "        for k in range(len(test_idx)):              \n",
    "            i_pos = test_start_pos + k              \n",
    "            future_pos = i_pos + h - 1\n",
    "            if future_pos >= len(full_df):\n",
    "                continue\n",
    "\n",
    "            start_pos = i_pos - window_size\n",
    "            if start_pos < 0:\n",
    "                continue\n",
    "\n",
    "            y_train = full_df[\"LOG_RETURN\"].iloc[start_pos:i_pos].copy()\n",
    "            if y_train.isna().any() or len(y_train) < 2 or y_train.var() == 0:\n",
    "                continue\n",
    "\n",
    "            # Theta needs a numeric index\n",
    "            y_train.index = pd.RangeIndex(0, len(y_train), 1)\n",
    "\n",
    "            try:\n",
    "                model = ThetaModel(y_train, period=1)  \n",
    "                fitted = model.fit()\n",
    "                forecast = fitted.forecast(steps=h).iloc[h - 1]\n",
    "            except Exception:\n",
    "                forecast = np.nan\n",
    "\n",
    "            true_value = full_df[\"LOG_RETURN\"].iloc[future_pos]\n",
    "\n",
    "            if pd.notna(forecast):\n",
    "                all_preds.append(forecast)\n",
    "                all_trues.append(true_value)\n",
    "\n",
    "                if fold == n_splits - 1:\n",
    "                    ts = full_df.index[future_pos]\n",
    "                    last_fold_preds.append((ts, forecast))\n",
    "                    last_fold_index.append(ts)\n",
    "\n",
    "    # Evaluation\n",
    "    if all_preds:\n",
    "        rmse = np.sqrt(mean_squared_error(all_trues, all_preds))\n",
    "        mse = mean_squared_error(all_trues, all_preds)\n",
    "        mae = mean_absolute_error(all_trues, all_preds)\n",
    "        results.append({\"Horizon\": h, \"Model\": \"Theta\", \"RMSE\": rmse, \"MSE\": mse, \"MAE\": mae})\n",
    "        print(f\"RMSE: {rmse:.5f}, MSE: {mse:.5f}, MAE: {mae:.5f}\")\n",
    "\n",
    "        # Plot last fold\n",
    "        pred_series = pd.Series(dict(last_fold_preds)).sort_index()\n",
    "\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(data.index, data[\"LOG_RETURN\"], label=\"True Returns\")\n",
    "        plt.plot(pred_series.index, pred_series.values, label=\"Theta Forecast\")\n",
    "\n",
    "        if pred_series.index.size > 0:\n",
    "            plt.axvline(pred_series.index[0], linestyle='--', label=\"Last Fold Start\")\n",
    "\n",
    "        plt.title(f\"Theta Forecast ({h}-Week Ahead) â€“ Last Fold\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Log Return\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Final Results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n Final Theta Forecast Evaluation \")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93518cf1",
   "metadata": {},
   "source": [
    "# Random Forest with fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4a17e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\", \"signal\"])\n",
    "\n",
    "# Create lag features\n",
    "max_return_lag = 3\n",
    "max_signal_lag = 3\n",
    "\n",
    "for lag in range(1, max_return_lag + 1):\n",
    "    data[f'return_lag{lag}'] = data['LOG_RETURN'].shift(lag)\n",
    "for lag in range(1, max_signal_lag + 1):\n",
    "    data[f'signal_lag{lag}'] = data['signal'].shift(lag)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Parameters\n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Results storage \n",
    "true_vals_rf = []\n",
    "preds_rf = []\n",
    "last_fold_preds_rf = []\n",
    "last_fold_true_rf = []\n",
    "last_fold_index_rf = []\n",
    "\n",
    "#Lag Optimization Grids\n",
    "return_lag_grid = [1, 2, 3]\n",
    "signal_lag_grid = [0, 1, 2,3]\n",
    "\n",
    "#Hyperparameter Options\n",
    "hyperparameter_grid = [(3, 2, 1),(5, 5, 2),(None, 2, 1)]\n",
    "\n",
    "best_rmse_rf = np.inf\n",
    "best_mse_rf = np.inf\n",
    "best_mae_rf = np.inf\n",
    "best_model_rf = None\n",
    "best_config_rf = None\n",
    "best_feature_importance_rf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ac0a802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grid Search over lag combinations and hyperparameters\n",
    "for n_return_lags in return_lag_grid:\n",
    "    for n_signal_lags in signal_lag_grid:\n",
    "        selected_features = [f'return_lag{lag}' for lag in range(1, n_return_lags + 1)] + \\\n",
    "                            [f'signal_lag{lag}' for lag in range(1, n_signal_lags + 1)] + ['signal_scaled']\n",
    "\n",
    "        for max_depth, min_samples_split, min_samples_leaf in hyperparameter_grid:\n",
    "            true_vals_rf = []\n",
    "            preds_rf = []\n",
    "            last_fold_preds_rf = []\n",
    "            last_fold_true_rf = []\n",
    "            last_fold_index_rf = []\n",
    "            importances_this_rf = []\n",
    "\n",
    "            for fold, (train_idx, test_idx) in enumerate(tscv.split(data)):\n",
    "                for i in test_idx:\n",
    "                    if i - window_size < 0:\n",
    "                            continue\n",
    "\n",
    "                    window_data = data.iloc[i - window_size:i+1].copy()\n",
    "                    train_window = window_data.iloc[:-1].copy()\n",
    "                    test_point = window_data.iloc[-1:].copy()\n",
    "\n",
    "                    # Scale signal to match log return std. All inputs will be expressed in same scale.\n",
    "                    log_std = train_window[\"LOG_RETURN\"].std()\n",
    "                    signal_std = train_window[\"signal\"].std()\n",
    "                    scale_factor = log_std / signal_std if signal_std > 0 else 1.0\n",
    "                    train_window[\"signal_scaled\"] = train_window[\"signal\"] * scale_factor\n",
    "                    test_point[\"signal_scaled\"] = test_point[\"signal\"] * scale_factor\n",
    "                    \n",
    "                    # Scale signal lags using the same factor \n",
    "                    for lag in range(1, n_signal_lags + 1):\n",
    "                        train_window[f'signal_lag{lag}'] *= scale_factor\n",
    "                        test_point[f'signal_lag{lag}'] *= scale_factor\n",
    "\n",
    "\n",
    "                    X_train = train_window[selected_features]\n",
    "                    X_test = test_point[selected_features]\n",
    "                    y_train = train_window[\"LOG_RETURN\"]\n",
    "                    y_test = test_point[\"LOG_RETURN\"].values[0]\n",
    "\n",
    "                    rf_model = RandomForestRegressor(n_estimators=100,max_depth=max_depth,\n",
    "                                                     min_samples_split=min_samples_split,\n",
    "                                                     min_samples_leaf=min_samples_leaf,random_state=42)\n",
    "                    rf_model.fit(X_train, y_train)\n",
    "                    pred_rf = rf_model.predict(X_test)[0]\n",
    "\n",
    "                    true_vals_rf.append(y_test)\n",
    "                    preds_rf.append(pred_rf)\n",
    "                    importances_this_rf.append(rf_model.feature_importances_)\n",
    "\n",
    "                    if fold == tscv.n_splits - 1:\n",
    "                        last_fold_preds_rf.append(pred_rf)\n",
    "                        last_fold_true_rf.append(y_test)\n",
    "                        last_fold_index_rf.append(data.index[i])\n",
    "\n",
    "            # Evaluation\n",
    "            rmse_rf = np.sqrt(mean_squared_error(true_vals_rf, preds_rf))\n",
    "            mse_rf = mean_squared_error(true_vals_rf, preds_rf)\n",
    "            mae_rf = mean_absolute_error(true_vals_rf, preds_rf)\n",
    "\n",
    "\n",
    "            if rmse_rf < best_rmse_rf:\n",
    "                best_rmse_rf = rmse_rf\n",
    "                best_mse_rf=mse_rf\n",
    "                best_mae_rf=mae_rf        \n",
    "                best_model_rf = rf_model\n",
    "                best_config_rf = (n_return_lags, n_signal_lags, max_depth, min_samples_split, min_samples_leaf)\n",
    "                best_preds_rf = last_fold_preds_rf\n",
    "                best_index_rf = last_fold_index_rf\n",
    "                importance_df = pd.DataFrame(importances_this_rf, columns=X_train.columns)\n",
    "                best_feature_importance_rf = importance_df.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b669e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best RF Config: return_lags={best_config_rf[0]}, signal_lags={best_config_rf[1]}, \"\n",
    "      f\"max_depth={best_config_rf[2]}, min_samples_split={best_config_rf[3]}, min_samples_leaf={best_config_rf[4]}\")\n",
    "\n",
    "print(f\"Best RF RMSE: {best_rmse_rf:.5f} MSE: {best_mse_rf:.5f} MAE: {best_mae_rf:.5f}\")\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "print(best_feature_importance_rf)\n",
    "\n",
    "full_preds_rf = np.full(len(data), np.nan)\n",
    "pred_positions_rf = data.index.get_indexer(best_index_rf)\n",
    "full_preds_rf[pred_positions_rf] = best_preds_rf\n",
    "\n",
    "true_returns_rf = data[\"LOG_RETURN\"].values\n",
    "dates = data.index\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(dates, true_returns_rf, label=\"True Returns\", color=\"blue\")\n",
    "plt.plot(dates, full_preds_rf, label=\"RF Forecast (last fold)\", color=\"orange\")\n",
    "plt.axvline(dates[pred_positions_rf[0]], color=\"black\", linestyle=\"--\", label=\"Last Fold Start\")\n",
    "\n",
    "plt.title(\"Random Forest Forecast (TimeSeriesSplit - Last Fold)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Log Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21266413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d5b5a3",
   "metadata": {},
   "source": [
    "# Random Forest for multiple horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6e6ae829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "forecast_horizons = [1, 5, 22, 65]\n",
    "n_return_lags = 3\n",
    "n_signal_lags = 1\n",
    "\n",
    "rf_params = {'n_estimators': 100,'max_depth': 5,'min_samples_split': 5,'min_samples_leaf': 2,'random_state': 42}\n",
    "\n",
    "#Data Preparation\n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\", \"signal\"])\n",
    "\n",
    "# Create lagged features\n",
    "for lag in range(1, n_return_lags + 1):\n",
    "    data[f'return_lag{lag}'] = data['LOG_RETURN'].shift(lag)\n",
    "for lag in range(1, n_signal_lags + 1):\n",
    "    data[f'signal_lag{lag}'] = data['signal'].shift(lag)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Feature columns\n",
    "selected_features = [f'return_lag{lag}' for lag in range(1, n_return_lags + 1)]\n",
    "selected_features += [f'signal_lag{lag}' for lag in range(1, n_signal_lags + 1)]\n",
    "selected_features += ['signal_scaled']\n",
    "\n",
    "# Forecasting Loop\n",
    "results = []\n",
    "# Collect feature importances for each horizon\n",
    "feature_importance_dict = {h: [] for h in forecast_horizons}\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "for horizon in forecast_horizons:\n",
    "    print(f\"\\nForecasting h={horizon} steps ahead\")\n",
    "    true_vals, preds = [], []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(data)):\n",
    "        full_df = data.iloc[np.concatenate([train_idx, test_idx])].copy()\n",
    "\n",
    "        for i in test_idx:\n",
    "            future_i = i + horizon - 1\n",
    "            if future_i >= len(full_df) or i - window_size < 0:\n",
    "                continue\n",
    "\n",
    "            window_data = full_df.iloc[i - window_size:i + 1].copy()\n",
    "            train_window = window_data.iloc[:-1].copy()\n",
    "            test_point = window_data.iloc[-1:].copy()\n",
    "\n",
    "            # Scale signal\n",
    "            log_std = train_window[\"LOG_RETURN\"].std()\n",
    "            signal_std = train_window[\"signal\"].std()\n",
    "            scale_factor = log_std / signal_std if signal_std > 0 else 1.0\n",
    "            train_window[\"signal_scaled\"] = train_window[\"signal\"] * scale_factor\n",
    "            test_point[\"signal_scaled\"] = test_point[\"signal\"] * scale_factor\n",
    "\n",
    "            for lag in range(1, n_signal_lags + 1):\n",
    "                train_window[f\"signal_lag{lag}\"] *= scale_factor\n",
    "                test_point[f\"signal_lag{lag}\"] *= scale_factor\n",
    "\n",
    "            X_train = train_window[selected_features]\n",
    "            y_train = train_window[\"LOG_RETURN\"]\n",
    "            X_test = test_point[selected_features]\n",
    "            y_test = full_df.iloc[future_i][\"LOG_RETURN\"]\n",
    "\n",
    "            model = RandomForestRegressor(**rf_params)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)[0]\n",
    "            # Collect feature importances\n",
    "            feature_importance_dict[horizon].append(model.feature_importances_)\n",
    "\n",
    "            true_vals.append(y_test)\n",
    "            preds.append(pred)\n",
    "\n",
    "    # Evaluation\n",
    "    rmse = np.sqrt(mean_squared_error(true_vals, preds))\n",
    "    mse = mean_squared_error(true_vals, preds)\n",
    "    mae = mean_absolute_error(true_vals, preds)\n",
    "\n",
    "    print(f\"RMSE: {rmse:.5f} | MSE: {mse:.5f} | MAE: {mae:.5f}\")\n",
    "\n",
    "    results.append({\"Horizon\": horizon,\"RMSE\": rmse,\"MSE\": mse,ss\"MAE\": mae})\n",
    "\n",
    "# Final Summary\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n Final Evaluation Summary\")\n",
    "print(results_df)\n",
    "# Compute and print average feature importances per horizon\n",
    "print(\"\\nAverage Feature Importances Across Horizons:\\n\")\n",
    "\n",
    "for h in forecast_horizons:\n",
    "    importance_array = np.array(feature_importance_dict[h])\n",
    "    mean_importance = importance_array.mean(axis=0)\n",
    "    feature_importance_df = pd.Series(mean_importance, index=selected_features)\n",
    "    print(f\"Horizon = {h} weeks ahead:\")\n",
    "    print(feature_importance_df.sort_values(ascending=False), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3714cb70",
   "metadata": {},
   "source": [
    "# SVM with fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "255a678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\", \"signal\"])\n",
    "\n",
    "# Parameter Grid \n",
    "param_grid = {'n_return_lags': [1, 2, 3],'n_signal_lags': [0, 1, 2, 3],'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.001, 0.01, 0.1],'kernel': ['rbf'] }  # Can include 'linear', 'poly' etc. if needed}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "# Rolling Forecast Setup \n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "best_rmse_svr = np.inf\n",
    "best_mse_svr = np.inf    \n",
    "best_mae_svr = np.inf   \n",
    "best_model_svr = None\n",
    "best_config_svr = None\n",
    "best_preds_svr = []\n",
    "best_index_svr = []\n",
    "\n",
    "#Tuning Loop\n",
    "for params in param_combinations:\n",
    "    df = data.copy()\n",
    "    for lag in range(1, params['n_return_lags'] + 1):\n",
    "        df[f'return_lag{lag}'] = df['LOG_RETURN'].shift(lag)\n",
    "    for lag in range(1, params['n_signal_lags'] + 1):\n",
    "        df[f'signal_lag{lag}'] = df['signal'].shift(lag)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    selected_features = [f'return_lag{lag}' for lag in range(1, params['n_return_lags'] + 1)] + \\\n",
    "                         [f'signal_lag{lag}' for lag in range(1, params['n_signal_lags'] + 1)] + ['signal_scaled']\n",
    "\n",
    "    true_vals_svr, preds_svr, last_fold_preds, last_fold_index = [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df)):\n",
    "        for i in test_idx:\n",
    "            if i - window_size < 0:\n",
    "                continue\n",
    "\n",
    "            window_data = df.iloc[i - window_size:i+1].copy()\n",
    "            train_window = window_data.iloc[:-1].copy()\n",
    "            test_point = window_data.iloc[-1:].copy()\n",
    "\n",
    "            # Scale signal to match log return std\n",
    "            log_std = train_window[\"LOG_RETURN\"].std()\n",
    "            signal_std = train_window[\"signal\"].std()\n",
    "            scale_factor = log_std / signal_std if signal_std > 0 else 1.0\n",
    "\n",
    "            train_window[\"signal_scaled\"] = train_window[\"signal\"] * scale_factor\n",
    "            test_point[\"signal_scaled\"] = test_point[\"signal\"] * scale_factor\n",
    "\n",
    "            for lag in range(1, params['n_signal_lags'] + 1):\n",
    "                train_window[f\"signal_lag{lag}\"] *= scale_factor\n",
    "                test_point[f\"signal_lag{lag}\"] *= scale_factor\n",
    "\n",
    "            X_train = train_window[selected_features]\n",
    "            y_train = train_window[\"LOG_RETURN\"]\n",
    "            X_test = test_point[selected_features]\n",
    "            y_test = test_point[\"LOG_RETURN\"].values[0]\n",
    "\n",
    "            model_svr = SVR(\n",
    "                kernel=params['kernel'],\n",
    "                C=params['C'],\n",
    "                epsilon=params['epsilon'])\n",
    "            model_svr.fit(X_train, y_train)\n",
    "            pred_svr = model_svr.predict(X_test)[0]\n",
    "\n",
    "            true_vals_svr.append(y_test)\n",
    "            preds_svr.append(pred_svr)\n",
    "\n",
    "            if fold == tscv.n_splits - 1:\n",
    "                last_fold_preds.append(pred_svr)\n",
    "                last_fold_index.append(df.index[i])\n",
    "\n",
    "    # Evaluation\n",
    "    rmse_svr = np.sqrt(mean_squared_error(true_vals_svr, preds_svr))\n",
    "    mse_svr = mean_squared_error(true_vals_svr, preds_svr)\n",
    "    mae_svr = mean_absolute_error(true_vals_svr, preds_svr)\n",
    "    \n",
    "    if rmse_svr < best_rmse_svr:\n",
    "        best_rmse_svr = rmse_svr\n",
    "        best_mse_svr = mse_svr\n",
    "        best_mae_svr = mae_svr    \n",
    "        best_model_svr = model_svr\n",
    "        best_config_svr = params\n",
    "        best_preds_svr = last_fold_preds\n",
    "        best_index_svr = last_fold_index\n",
    "\n",
    "# Create predictions DataFrame\n",
    "best_results_df = pd.DataFrame({'Date': best_index_svr, 'Prediction': best_preds_svr})\n",
    "best_results_df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b75ad847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "print(\"Best SVM Config:\", best_config_svr)\n",
    "print(f\"Best RMSE: {best_rmse_svr:.5f} MSE: {best_mse_svr:.5f} MAE: {best_mae_svr:.5f}\")\n",
    "\n",
    "# Plot\n",
    "full_preds_svr = np.full(len(data), np.nan)\n",
    "pred_positions = data.index.get_indexer(best_index_svr)\n",
    "full_preds_svr[pred_positions] = best_preds_svr\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(data.index, data[\"LOG_RETURN\"].values, label=\"True Returns\", color=\"blue\")\n",
    "plt.plot(data.index, full_preds_svr, label=\"SVM Forecast (last fold)\", color=\"orange\")\n",
    "plt.axvline(data.index[pred_positions[0]], color=\"black\", linestyle=\"--\", label=\"Last Fold Start\")\n",
    "\n",
    "plt.title(\"SVM Forecast (Rolling Window Last Fold)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Log Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79dd417",
   "metadata": {},
   "source": [
    "# SVM for multiple horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cfeedcdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#CONFIG \n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "forecast_horizons = [5, 22, 65]\n",
    "random_state = 42\n",
    "\n",
    "# Best Parameters (from earlier tuning on 1-step ahead)\n",
    "svr_params = {'kernel': 'rbf','C': 0.1,'epsilon': 0.01}\n",
    "\n",
    "# Load and Prepare Data \n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\", \"signal\"])\n",
    "\n",
    "# Feature Engineering\n",
    "n_return_lags = 3\n",
    "n_signal_lags = 2\n",
    "\n",
    "for lag in range(1, n_return_lags + 1):\n",
    "    data[f'return_lag{lag}'] = data['LOG_RETURN'].shift(lag)\n",
    "for lag in range(1, n_signal_lags + 1):\n",
    "    data[f'signal_lag{lag}'] = data['signal'].shift(lag)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Feature list\n",
    "selected_features = [f'return_lag{lag}' for lag in range(1, n_return_lags + 1)] + \\\n",
    "                    [f'signal_lag{lag}' for lag in range(1, n_signal_lags + 1)] + \\\n",
    "                    ['signal_scaled']\n",
    "\n",
    "# Cross-validation splits\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "splits = list(tscv.split(data))\n",
    "\n",
    "# Forecasting Loop \n",
    "results = []\n",
    "\n",
    "for horizon in forecast_horizons:\n",
    "    print(f\"\\nSVM Forecasting Horizon: {horizon} steps ahead\")\n",
    "\n",
    "    true_vals = []\n",
    "    preds = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in tqdm(enumerate(splits), total=n_splits, desc=f\"H={horizon}\"):\n",
    "        full_df = data.iloc[np.concatenate([train_idx, test_idx])].copy()\n",
    "\n",
    "        for i in test_idx:\n",
    "            future_i = i + horizon - 1\n",
    "            if i - window_size < 0 or future_i >= len(full_df):\n",
    "                continue\n",
    "\n",
    "            window = full_df.iloc[i - window_size:i + 1].copy()\n",
    "            train_window = window.iloc[:-1].copy()\n",
    "            test_point = window.iloc[-1:].copy()\n",
    "\n",
    "            # Scale signal\n",
    "            log_std = train_window[\"LOG_RETURN\"].std()\n",
    "            signal_std = train_window[\"signal\"].std()\n",
    "            scale_factor = log_std / signal_std if signal_std > 0 else 1.0\n",
    "            train_window[\"signal_scaled\"] = train_window[\"signal\"] * scale_factor\n",
    "            test_point[\"signal_scaled\"] = test_point[\"signal\"] * scale_factor\n",
    "            for lag in range(1, n_signal_lags + 1):\n",
    "                train_window[f\"signal_lag{lag}\"] *= scale_factor\n",
    "                test_point[f\"signal_lag{lag}\"] *= scale_factor\n",
    "\n",
    "            X_train = train_window[selected_features]\n",
    "            y_train = train_window[\"LOG_RETURN\"]\n",
    "            X_test = test_point[selected_features]\n",
    "            y_test = full_df.iloc[future_i][\"LOG_RETURN\"]\n",
    "\n",
    "            # Fit and predict\n",
    "            model = SVR(**svr_params)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)[0]\n",
    "\n",
    "            preds.append(pred)\n",
    "            true_vals.append(y_test)\n",
    "\n",
    "    # Evaluation \n",
    "    rmse = np.sqrt(mean_squared_error(true_vals, preds))\n",
    "    mse = mean_squared_error(true_vals, preds)\n",
    "    mae = mean_absolute_error(true_vals, preds)\n",
    "\n",
    "    print(f\"Horizon {horizon}: RMSE = {rmse:.5f}, MSE = {mse:.5f}, MAE = {mae:.5f}\")\n",
    "    results.append({\"Horizon\": horizon,\"RMSE\": rmse,\"MSE\": mse,\"MAE\": mae })\n",
    "\n",
    "# Final Results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSVM Forecasting Results Summary:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77382a5",
   "metadata": {},
   "source": [
    "# XGBoost with fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\", \"signal\"])\n",
    "\n",
    "# Parameter Grid \n",
    "param_grid = {'n_return_lags': [1, 2, 3],'n_signal_lags': [0, 1, 2, 3],'max_depth': [3, 5],\n",
    "              'learning_rate': [0.01, 0.05],'n_estimators': [100],'subsample': [0.8],\n",
    "              'colsample_bytree': [0.8],'min_child_weight': [1],s'gamma': [0]}\n",
    "keys, values = zip(*param_grid.items())\n",
    "param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "# Rolling Forecast Setup \n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "best_rmse = np.inf\n",
    "best_mse = np.inf\n",
    "best_mae = np.inf\n",
    "best_model = None\n",
    "best_config = None\n",
    "best_preds = []\n",
    "best_index = []\n",
    "best_feature_importance_xgb=[]\n",
    "\n",
    "# Tuning Loop\n",
    "for params in param_combinations:\n",
    "    # Create lag features\n",
    "    df = data.copy()\n",
    "    for lag in range(1, params['n_return_lags'] + 1):\n",
    "        df[f'return_lag{lag}'] = df['LOG_RETURN'].shift(lag)\n",
    "    for lag in range(1, params['n_signal_lags'] + 1):\n",
    "        df[f'signal_lag{lag}'] = df['signal'].shift(lag)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    selected_features = [f'return_lag{lag}' for lag in range(1, params['n_return_lags'] + 1)] + \\\n",
    "                         [f'signal_lag{lag}' for lag in range(1, params['n_signal_lags'] + 1)] + ['signal_scaled']\n",
    "\n",
    "    true_vals, preds, last_fold_preds, last_fold_index = [], [], [], []\n",
    "    importances_xgb=[]\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df)):\n",
    "        for i in test_idx:\n",
    "            if i - window_size < 0:\n",
    "                continue\n",
    "\n",
    "            window_data = df.iloc[i - window_size:i+1].copy()\n",
    "            train_window = window_data.iloc[:-1].copy()\n",
    "            test_point = window_data.iloc[-1:].copy()\n",
    "\n",
    "            # Scale signal to log return std\n",
    "            log_std = train_window[\"LOG_RETURN\"].std()\n",
    "            signal_std = train_window[\"signal\"].std()\n",
    "            scale_factor = log_std / signal_std if signal_std > 0 else 1.0\n",
    "\n",
    "            train_window[\"signal_scaled\"] = train_window[\"signal\"] * scale_factor\n",
    "            test_point[\"signal_scaled\"] = test_point[\"signal\"] * scale_factor\n",
    "\n",
    "            for lag in range(1, params['n_signal_lags'] + 1):\n",
    "                train_window[f\"signal_lag{lag}\"] *= scale_factor\n",
    "                test_point[f\"signal_lag{lag}\"] *= scale_factor\n",
    "\n",
    "            X_train = train_window[selected_features]\n",
    "            y_train = train_window[\"LOG_RETURN\"]\n",
    "            X_test = test_point[selected_features]\n",
    "            y_test = test_point[\"LOG_RETURN\"].values[0]\n",
    "\n",
    "            model = XGBRegressor(objective='reg:squarederror',n_estimators=params['n_estimators'],\n",
    "                                 max_depth=params['max_depth'],learning_rate=params['learning_rate'],\n",
    "                                 subsample=params['subsample'],colsample_bytree=params['colsample_bytree'],\n",
    "                                 min_child_weight=params['min_child_weight'],gamma=params['gamma'],random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)[0]\n",
    "            importances_xgb.append(model.feature_importances_)\n",
    "\n",
    "            true_vals.append(y_test)\n",
    "            preds.append(pred)\n",
    "\n",
    "            if fold == tscv.n_splits - 1:\n",
    "                last_fold_preds.append(pred)\n",
    "                last_fold_index.append(df.index[i])\n",
    "\n",
    "    # Evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(true_vals, preds))\n",
    "    mse = mean_squared_error(true_vals, preds)\n",
    "    mae= mean_absolute_error(true_vals, preds)   \n",
    "    \n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_mse = mse\n",
    "        best_mae = mae\n",
    "        best_model = model\n",
    "        best_config = params\n",
    "        best_preds = last_fold_preds\n",
    "        best_index = last_fold_index\n",
    "        importance_xgb = pd.DataFrame(importances_xgb, columns=X_train.columns)\n",
    "        best_feature_importance_xgb = importance_xgb.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00e7f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "print(\"Best XGBoost Config:\", best_config)\n",
    "print(f\"Best RMSE: {best_rmse:.5f} MSE: {best_mse:.5f} MAE: {best_mae:.5f}\")\n",
    "print(\"Feature Importances:\")\n",
    "print(best_feature_importance_xgb)\n",
    "\n",
    "\n",
    "# Plot\n",
    "full_preds = np.full(len(data), np.nan)\n",
    "pred_positions = data.index.get_indexer(best_index)\n",
    "full_preds[pred_positions] = best_preds\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(data.index, data[\"LOG_RETURN\"].values, label=\"True Returns\", color=\"blue\")\n",
    "plt.plot(data.index, full_preds, label=\"XGBoost Forecast (last fold)\", color=\"orange\")\n",
    "plt.axvline(data.index[pred_positions[0]], color=\"black\", linestyle=\"--\", label=\"Last Fold Start\")\n",
    "\n",
    "plt.title(\"XGBoost Forecast (Rolling Window Last Fold)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Log Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f3e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e5c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a180499a",
   "metadata": {},
   "source": [
    "# Hybrid ARIMA and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5deafad3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell omitted for brevity in cleaned version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9632458",
   "metadata": {},
   "source": [
    "# Hybrid ARIMA and XGBoost for multiple forecasting horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e24770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell omitted for brevity in cleaned version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931297b",
   "metadata": {},
   "source": [
    "# XGBoost including return lags, current signal , sentiment (current + lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62d416e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell omitted for brevity in cleaned version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2934dfd7",
   "metadata": {},
   "source": [
    "# XGBoost without signal variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9637d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost_models(merged_rolling, best_params_signal):\n",
    "    window_size = 120\n",
    "    n_splits = 5\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_type in ['with_signal', 'without_signal']:\n",
    "        data = merged_rolling.copy()\n",
    "\n",
    "        if model_type == 'with_signal':\n",
    "            data = data.dropna(subset=[\"LOG_RETURN\", \"signal\"])\n",
    "        else:\n",
    "            data = data.dropna(subset=[\"LOG_RETURN\"])\n",
    "\n",
    "        # Create lag features\n",
    "        for lag in range(1, best_params_signal['n_return_lags'] + 1):\n",
    "            data[f'return_lag{lag}'] = data['LOG_RETURN'].shift(lag)\n",
    "\n",
    "        if model_type == 'with_signal':\n",
    "            for lag in range(1, best_params_signal['n_signal_lags'] + 1):\n",
    "                data[f'signal_lag{lag}'] = data['signal'].shift(lag)\n",
    "        data.dropna(inplace=True)\n",
    "\n",
    "        # Feature list\n",
    "        selected_features = [f'return_lag{lag}' for lag in range(1, best_params_signal['n_return_lags'] + 1)]\n",
    "        if model_type == 'with_signal':\n",
    "            selected_features += [f'signal_lag{lag}' for lag in range(1, best_params_signal['n_signal_lags'] + 1)]\n",
    "            selected_features += ['signal_scaled']\n",
    "\n",
    "        true_vals, preds = [], []\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(tscv.split(data)):\n",
    "            for i in test_idx:\n",
    "                if i - window_size < 0:\n",
    "                    continue\n",
    "\n",
    "                window_data = data.iloc[i - window_size:i + 1].copy()\n",
    "                train_window = window_data.iloc[:-1].copy()\n",
    "                test_point = window_data.iloc[-1:].copy()\n",
    "\n",
    "                if model_type == 'with_signal':\n",
    "                    # Scale signal\n",
    "                    log_std = train_window[\"LOG_RETURN\"].std()\n",
    "                    signal_std = train_window[\"signal\"].std()\n",
    "                    scale_factor = log_std / signal_std if signal_std > 0 else 1.0\n",
    "\n",
    "                    train_window[\"signal_scaled\"] = train_window[\"signal\"] * scale_factor\n",
    "                    test_point[\"signal_scaled\"] = test_point[\"signal\"] * scale_factor\n",
    "\n",
    "                    for lag in range(1, best_params_signal['n_signal_lags'] + 1):\n",
    "                        train_window[f\"signal_lag{lag}\"] *= scale_factor\n",
    "                        test_point[f\"signal_lag{lag}\"] *= scale_factor\n",
    "\n",
    "                X_train = train_window[selected_features]\n",
    "                y_train = train_window[\"LOG_RETURN\"]\n",
    "                X_test = test_point[selected_features]\n",
    "                y_test = test_point[\"LOG_RETURN\"].values[0]\n",
    "\n",
    "                model = XGBRegressor(\n",
    "                    objective='reg:squarederror',\n",
    "                    n_estimators=best_params_signal['n_estimators'],\n",
    "                    max_depth=best_params_signal['max_depth'],\n",
    "                    learning_rate=best_params_signal['learning_rate'],\n",
    "                    subsample=best_params_signal['subsample'],\n",
    "                    colsample_bytree=best_params_signal['colsample_bytree'],\n",
    "                    min_child_weight=best_params_signal['min_child_weight'],\n",
    "                    gamma=best_params_signal['gamma'],\n",
    "                    random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "                pred = model.predict(X_test)[0]\n",
    "\n",
    "                true_vals.append(y_test)\n",
    "                preds.append(pred)\n",
    "\n",
    "        results[model_type] = {\n",
    "            'true': np.array(true_vals),\n",
    "            'pred': np.array(preds),\n",
    "            'rmse': np.sqrt(mean_squared_error(true_vals, preds)),\n",
    "            'mae': mean_absolute_error(true_vals, preds)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc89a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (same optimal XGBoost parameters) \n",
    "params = {'n_return_lags': 1,'max_depth': 3,'learning_rate': 0.01,'n_estimators': 100,'subsample': 0.8,\n",
    "          'colsample_bytree': 0.8,'min_child_weight': 1,'gamma': 0}\n",
    "\n",
    "#  Data Preparation\n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\"])  \n",
    "\n",
    "# Create return lag features only\n",
    "for lag in range(1, params['n_return_lags'] + 1):\n",
    "    data[f'return_lag{lag}'] = data['LOG_RETURN'].shift(lag)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "selected_features = [f'return_lag{lag}' for lag in range(1, params['n_return_lags'] + 1)]\n",
    "\n",
    "# Rolling Forecast \n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "true_vals, preds, last_fold_preds, last_fold_index, importances_xgb = [], [], [], [], []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(data)):\n",
    "    for i in test_idx:\n",
    "        if i - window_size < 0:\n",
    "            continue\n",
    "\n",
    "        window_data = data.iloc[i - window_size:i + 1].copy()\n",
    "        train_window = window_data.iloc[:-1].copy()\n",
    "        test_point = window_data.iloc[-1:].copy()\n",
    "\n",
    "        X_train = train_window[selected_features]\n",
    "        y_train = train_window[\"LOG_RETURN\"]\n",
    "        X_test = test_point[selected_features]\n",
    "        y_test = test_point[\"LOG_RETURN\"].values[0]\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=params['n_estimators'],\n",
    "            max_depth=params['max_depth'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            subsample=params['subsample'],\n",
    "            colsample_bytree=params['colsample_bytree'],\n",
    "            min_child_weight=params['min_child_weight'],\n",
    "            gamma=params['gamma'],\n",
    "            random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)[0]\n",
    "        importances_xgb.append(model.feature_importances_)\n",
    "\n",
    "        true_vals.append(y_test)\n",
    "        preds.append(pred)\n",
    "\n",
    "        if fold == tscv.n_splits - 1:\n",
    "            last_fold_preds.append(pred)\n",
    "            last_fold_index.append(data.index[i])\n",
    "\n",
    "#  Evaluation\n",
    "rmse = np.sqrt(mean_squared_error(true_vals, preds))\n",
    "mse = mean_squared_error(true_vals, preds)\n",
    "mae = mean_absolute_error(true_vals, preds)\n",
    "\n",
    "print(\"XGBoost (Only Return Lags, No Signal) Forecast Results:\")\n",
    "print(f\"RMSE: {rmse:.5f}  MSE: {mse:.5f}  MAE: {mae:.5f}\")\n",
    "\n",
    "# Feature Importance\n",
    "importance_df = pd.DataFrame(importances_xgb, columns=X_train.columns)\n",
    "feature_importance_mean = importance_df.mean().sort_values(ascending=False)\n",
    "print(\"\\nMean Feature Importances:\")\n",
    "print(feature_importance_mean)\n",
    "\n",
    "# Plot Forecast (Last Fold) \n",
    "full_preds = np.full(len(data), np.nan)\n",
    "pred_positions = data.index.get_indexer(last_fold_index)\n",
    "full_preds[pred_positions] = last_fold_preds\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(data.index, data[\"LOG_RETURN\"].values, label=\"True Returns\", color=\"blue\")\n",
    "plt.plot(data.index, full_preds, label=\"XGBoost Forecast (last fold)\", color=\"orange\")\n",
    "plt.axvline(data.index[pred_positions[0]], color=\"black\", linestyle=\"--\", label=\"Last Fold Start\")\n",
    "\n",
    "plt.title(\"XGBoost Forecast Without Sentiment (Rolling Window Last Fold)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Log Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0ccecc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "params = {'n_return_lags': 1,'max_depth': 3,'learning_rate': 0.01,'n_estimators': 100,'subsample': 0.8,\n",
    "          'colsample_bytree': 0.8,'min_child_weight': 1,'gamma': 0}\n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "forecast_horizons = [5, 22, 65]\n",
    "\n",
    "# Prepare data\n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\"])\n",
    "\n",
    "results = []\n",
    "feature_importances_by_horizon = {}\n",
    "\n",
    "for h in forecast_horizons:\n",
    "    df = data.copy()\n",
    "    df[f'target_h{h}'] = df['LOG_RETURN'].shift(-h)\n",
    "\n",
    "    # Create lag features\n",
    "    for lag in range(1, params['n_return_lags'] + 1):\n",
    "        df[f'return_lag{lag}'] = df['LOG_RETURN'].shift(lag)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    selected_features = [f'return_lag{lag}' for lag in range(1, params['n_return_lags'] + 1)]\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    true_vals, preds, importances_xgb = [], [], []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df)):\n",
    "        for i in test_idx:\n",
    "            future_i = i + h - 1\n",
    "            if i - window_size < 0 or future_i >= len(df):\n",
    "                continue\n",
    "\n",
    "            window_data = df.iloc[i - window_size:i + 1].copy()\n",
    "            train_window = window_data.iloc[:-1].copy()\n",
    "            test_point = window_data.iloc[-1:].copy()\n",
    "\n",
    "            X_train = train_window[selected_features]\n",
    "            y_train = train_window[f'target_h{h}']\n",
    "            X_test = test_point[selected_features]\n",
    "            y_test = df.iloc[future_i][f\"target_h{h}\"]\n",
    "\n",
    "            model = XGBRegressor(objective='reg:squarederror',n_estimators=params['n_estimators'],\n",
    "                                 max_depth=params['max_depth'],learning_rate=params['learning_rate'],\n",
    "                                 subsample=params['subsample'],colsample_bytree=params['colsample_bytree'],\n",
    "                                 min_child_weight=params['min_child_weight'],gamma=params['gamma'],random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)[0]\n",
    "            importances_xgb.append(model.feature_importances_)\n",
    "\n",
    "            true_vals.append(y_test)\n",
    "            preds.append(pred)\n",
    "\n",
    "    # Evaluation\n",
    "    rmse = np.sqrt(mean_squared_error(true_vals, preds))\n",
    "    mse = mean_squared_error(true_vals, preds)\n",
    "    mae = mean_absolute_error(true_vals, preds)\n",
    "\n",
    "    results.append({\n",
    "        \"Horizon\": h,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae})\n",
    "\n",
    "    if importances_xgb:\n",
    "        imp_df = pd.DataFrame(importances_xgb, columns=selected_features)\n",
    "        feature_importances_by_horizon[h] = imp_df.mean().sort_values(ascending=False)\n",
    "\n",
    "# Output results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nEvaluation Results for XGBoost (No Signal):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d68014",
   "metadata": {},
   "source": [
    "# Random Forest without signal variable for multiple forecasting horizon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c5a223a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell omitted for brevity in cleaned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc61e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74cfbaa8",
   "metadata": {},
   "source": [
    "# SVM without signal variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63a9e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your data and drop rows with missing log returns only\n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\"])\n",
    "\n",
    "# Use the optimal number of return lags and fixed SVR parameters\n",
    "n_return_lags = 3\n",
    "C = 0.1\n",
    "epsilon = 0.01\n",
    "kernel = 'rbf'\n",
    "\n",
    "# Create return lags\n",
    "for lag in range(1, n_return_lags + 1):\n",
    "    data[f'return_lag{lag}'] = data['LOG_RETURN'].shift(lag)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "selected_features = [f'return_lag{lag}' for lag in range(1, n_return_lags + 1)]\n",
    "\n",
    "# Rolling Forecast Setup\n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "true_vals_svr, preds_svr, last_fold_preds, last_fold_index = [], [], [], []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(data)):\n",
    "    for i in test_idx:\n",
    "        if i - window_size < 0:\n",
    "            continue\n",
    "\n",
    "        window_data = data.iloc[i - window_size:i+1].copy()\n",
    "        train_window = window_data.iloc[:-1].copy()\n",
    "        test_point = window_data.iloc[-1:].copy()\n",
    "\n",
    "        X_train = train_window[selected_features]\n",
    "        y_train = train_window[\"LOG_RETURN\"]\n",
    "        X_test = test_point[selected_features]\n",
    "        y_test = test_point[\"LOG_RETURN\"].values[0]\n",
    "\n",
    "        model_svr = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "        model_svr.fit(X_train, y_train)\n",
    "        pred_svr = model_svr.predict(X_test)[0]\n",
    "\n",
    "        true_vals_svr.append(y_test)\n",
    "        preds_svr.append(pred_svr)\n",
    "\n",
    "        if fold == tscv.n_splits - 1:\n",
    "            last_fold_preds.append(pred_svr)\n",
    "            last_fold_index.append(data.index[i])\n",
    "\n",
    "# Evaluation\n",
    "rmse_svr = np.sqrt(mean_squared_error(true_vals_svr, preds_svr))\n",
    "mse_svr = mean_squared_error(true_vals_svr, preds_svr)\n",
    "mae_svr = mean_absolute_error(true_vals_svr, preds_svr)\n",
    "\n",
    "# Output Results\n",
    "print(\"SVR without Signal\")\n",
    "print(f\"RMSE: {rmse_svr:.5f} MSE: {mse_svr:.5f}  MAE: {mae_svr:.5f}\")\n",
    "\n",
    "# Plot (Last Fold Only)\n",
    "full_preds = np.full(len(data), np.nan)\n",
    "pred_positions = data.index.get_indexer(last_fold_index)\n",
    "full_preds[pred_positions] = last_fold_preds\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(data.index, data[\"LOG_RETURN\"].values, label=\"True Returns\", color=\"blue\")\n",
    "plt.plot(data.index, full_preds, label=\"SVM (last fold)\", color=\"orange\")\n",
    "plt.axvline(data.index[pred_positions[0]], color=\"black\", linestyle=\"--\", label=\"Last Fold Start\")\n",
    "\n",
    "plt.title(\"Random Forest Forecast Without Sentiment (Rolling Window Last Fold)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Log Return\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75be8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best SVM parameters and setup\n",
    "n_return_lags = 3\n",
    "C = 0.1\n",
    "epsilon = 0.01\n",
    "kernel = 'rbf'\n",
    "window_size = 120\n",
    "n_splits = 5\n",
    "forecast_horizons = [5, 22, 65]\n",
    "\n",
    "# Prepare data\n",
    "data = merged_rolling.copy().dropna(subset=[\"LOG_RETURN\"])\n",
    "for lag in range(1, n_return_lags + 1):\n",
    "    data[f'return_lag{lag}'] = data[\"LOG_RETURN\"].shift(lag)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "selected_features = [f'return_lag{lag}' for lag in range(1, n_return_lags + 1)]\n",
    "\n",
    "# Initialize storage\n",
    "results = {}\n",
    "\n",
    "# Run forecasts for each horizon\n",
    "for horizon in forecast_horizons:\n",
    "    print(f\"\\n Forecast Horizon: {horizon} weeks \")\n",
    "\n",
    "    true_vals_svr, preds_svr, last_fold_preds, last_fold_index = [], [], [], []\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(data)):\n",
    "        for i in test_idx:\n",
    "            future_i = i + horizon - 1\n",
    "            if i - window_size < 0 or future_i >= len(data):\n",
    "                continue\n",
    "\n",
    "            window_data = data.iloc[i - window_size:i + 1].copy()\n",
    "            train_window = window_data.iloc[:-1].copy()\n",
    "            test_point = window_data.iloc[-1:].copy()\n",
    "\n",
    "            X_train = train_window[selected_features]\n",
    "            y_train = train_window[\"LOG_RETURN\"]\n",
    "            X_test = test_point[selected_features]\n",
    "            y_test = data.iloc[future_i][\"LOG_RETURN\"]\n",
    "\n",
    "            model_svr = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "            model_svr.fit(X_train, y_train)\n",
    "            pred_svr = model_svr.predict(X_test)[0]\n",
    "\n",
    "            true_vals_svr.append(y_test)\n",
    "            preds_svr.append(pred_svr)\n",
    "\n",
    "            if fold == n_splits - 1:\n",
    "                last_fold_preds.append(pred_svr)\n",
    "                last_fold_index.append(data.index[future_i])\n",
    "\n",
    "    # Evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(true_vals_svr, preds_svr))\n",
    "    mse = mean_squared_error(true_vals_svr, preds_svr)\n",
    "    mae = mean_absolute_error(true_vals_svr, preds_svr)\n",
    "\n",
    "    print(f\"RMSE: {rmse:.5f}  MSE: {mse:.5f}  MAE: {mae:.5f}\")\n",
    "\n",
    "    results[horizon] = {\"RMSE\": rmse, \"MSE\": mse, \"MAE\": mae}\n",
    "\n",
    "    # Plot last fold\n",
    "    full_preds = np.full(len(data), np.nan)\n",
    "    pred_positions = data.index.get_indexer(last_fold_index)\n",
    "    full_preds[pred_positions] = last_fold_preds\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(data.index, data[\"LOG_RETURN\"].values, label=\"True Returns\", color=\"blue\")\n",
    "    plt.plot(data.index, full_preds, label=f\"SVM Forecast (Horizon {horizon})\", color=\"orange\")\n",
    "    if len(pred_positions) > 0:\n",
    "        plt.axvline(data.index[pred_positions[0]], color=\"black\", linestyle=\"--\", label=\"Last Fold Start\")\n",
    "\n",
    "    plt.title(f\"SVM Forecast Without Signal (Horizon: {horizon} weeks)\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Log Return\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator(1))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n Forecast Evaluation Summary\")\n",
    "summary_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "print(summary_df.round(5))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
